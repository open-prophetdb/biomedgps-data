{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"BioMedGPS Data \u00b6 A repo for building a knowledge graph and training knowledge graph embedding models for drug repurposing and disease mechanism research. Follow the DocWebsite to learn more about this project. This DocWebsite is built on all markdown files in this repository. So you can also read the markdown files in this repository to learn more about this project. Introduction \u00b6 A knowledge graph is a graph-structured database that contains entities and relations. The entities are the nodes in the graph and the relations are the edges in the graph. The knowledge graph can be used to represent the biomedical knowledge and the relations between entities. A biomedical knowledge graph can be used for drug repurposing and disease mechanism research. Such as: From Nicholson et al. CSBJ 2020. The knowledge graph can be used to train knowledge graph embedding models. From Nicholson et al. CSBJ 2020. But before that, we need to do some preprocessing to build a knowledge graph. Such as Entity Alignment , Entity Disambiguation . The following figure shows the key steps in the project. Unknown Source [TBD] Before you start, I recommend you to read the following papers: Barab\u00e1si, A.-L., Gulbahce, N. & Loscalzo, J. Network medicine: a network-based approach to human disease. Nat. Rev. Genet. 12, 56\u201368 (2011). Nicholson, David N., and Casey S. Greene. \"Constructing knowledge graphs and their biomedical applications.\" Computational and structural biotechnology journal 18 (2020): 1414-1428. Ioannidis, Vassilis N. and Song, Xiang and Manchanda, Saurav and Li, Mufei and Pan, Xiaoqin and Zheng, Da and Ning, Xia and Zeng, Xiangxiang and Karypis, George. DRKG - Drug Repurposing Knowledge Graph for Covid-19. PDF Key Steps in the Project \u00b6 If you want to use the pre-built knowledge graph and the pre-trained knowledge graph embedding models, you can skip the following steps and access our online service . If you only want to use and analyze the pre-built knowledge graph, you can follow the instructions in the README.md file to download the pre-built knowledge graph. After that, you can see the graph_analysis directory to analyze the knowledge graph. If you are interested in how the training scripts work, you can see the biomedgps-model repository. Please note that it is not necessary to run all the following steps in the project. You can run the steps you are interested in. But you need to make sure the dependencies among the steps. For example, if you want to train the knowledge graph embedding models, you need to build a knowledge graph or download the pre-built knowledge graph first. If you want to analyze the knowledge graph embedding models, you need to train the knowledge graph embedding models first. Step 1: Install dependencies \u00b6 More details can be found in the Install Dependencies file. Step 2: Build & Analyze a knowledge graph \u00b6 This repository contains the codes to build a knowledge graph for BioMedGPS project. Which depends on the ontology-matcher package and graph-builder package. If you want to run the following codes to build a knowledge graph for BioMedGPS project, you need to install all dependencies first. Please see the Install Dependencies file. After that, you can run the following codes to build a knowledge graph for BioMedGPS project. NOTE: Be sure to activate the python environment you created and located in the root directory of this repository when running the following codes. # Remove the following directories for a clean build rm -rf ./graph_data/extracted_entities ./graph_data/formatted_entities ./graph_data/formatted_relations python run_markdown.py ./graph_data/KG_README.md --run-all # The run_markdown.py is a script to run the codes in a markdown file. # It will extract the code blocks from the markdown file and run them one by one. # If you want to run a specific code block, you can use the following command. # If you see 'Cannot identify the language' message, this means that the code block is not necessary to run. python run_markdown.py ./graph_data/KG_README.md If you want to build a knowledge graph for BioMedGPS project step by step by yourself, you can follow the instructions in the KG_README.md file. How to analyze the knowledge graph? More details can be found in the graph_analysis directory in this repository or see the related documentation graph_analysis/README.md . [Deprecated] Step 3: Train, Evaluate, Analyze & Benchmark KGE models \u00b6 NOTE: We're building a new repo for training, evaluating, analyzing and benchmarking knowledge graph embedding / GNN models. Please follow the biomedgps-model repository for the latest updates. Train & evaluate knowledge graph embedding models If you want to train the knowledge graph embedding models by yourself, you can see the training_kge directory in this repository or see the related documentation training_kge/README.md . Benchmark knowledge graph embedding models If you want to benchmark the knowledge graph embedding models, you can see the benchmarks directory in this repository or see the related documentation benchmarks/README.md . Analyze the knowledge graph embedding models If you want to analyze the knowledge graph embedding models, you can see the embedding directory in this repository or see the related documentation embedding/README.md . [Deprecated] Step 4: Link Prediction \u00b6 NOTE: We're building a new repo for link prediction. Please follow the biomedgps-model repository for the latest updates. If you want to predict the relations between entities, you can see the prediction directory in this repository or see the related documentation prediction/README.md . [Deprecated] Step 5: Explain the prediction results \u00b6 NOTE: We're building a new repo for explaining the prediction results. Please follow the biomedgps-model repository for the latest updates. More details can be found in the biomedgps-explainer repository.","title":"Introduction"},{"location":"#biomedgps-data","text":"A repo for building a knowledge graph and training knowledge graph embedding models for drug repurposing and disease mechanism research. Follow the DocWebsite to learn more about this project. This DocWebsite is built on all markdown files in this repository. So you can also read the markdown files in this repository to learn more about this project.","title":"BioMedGPS Data"},{"location":"#introduction","text":"A knowledge graph is a graph-structured database that contains entities and relations. The entities are the nodes in the graph and the relations are the edges in the graph. The knowledge graph can be used to represent the biomedical knowledge and the relations between entities. A biomedical knowledge graph can be used for drug repurposing and disease mechanism research. Such as: From Nicholson et al. CSBJ 2020. The knowledge graph can be used to train knowledge graph embedding models. From Nicholson et al. CSBJ 2020. But before that, we need to do some preprocessing to build a knowledge graph. Such as Entity Alignment , Entity Disambiguation . The following figure shows the key steps in the project. Unknown Source [TBD] Before you start, I recommend you to read the following papers: Barab\u00e1si, A.-L., Gulbahce, N. & Loscalzo, J. Network medicine: a network-based approach to human disease. Nat. Rev. Genet. 12, 56\u201368 (2011). Nicholson, David N., and Casey S. Greene. \"Constructing knowledge graphs and their biomedical applications.\" Computational and structural biotechnology journal 18 (2020): 1414-1428. Ioannidis, Vassilis N. and Song, Xiang and Manchanda, Saurav and Li, Mufei and Pan, Xiaoqin and Zheng, Da and Ning, Xia and Zeng, Xiangxiang and Karypis, George. DRKG - Drug Repurposing Knowledge Graph for Covid-19. PDF","title":"Introduction"},{"location":"#key-steps-in-the-project","text":"If you want to use the pre-built knowledge graph and the pre-trained knowledge graph embedding models, you can skip the following steps and access our online service . If you only want to use and analyze the pre-built knowledge graph, you can follow the instructions in the README.md file to download the pre-built knowledge graph. After that, you can see the graph_analysis directory to analyze the knowledge graph. If you are interested in how the training scripts work, you can see the biomedgps-model repository. Please note that it is not necessary to run all the following steps in the project. You can run the steps you are interested in. But you need to make sure the dependencies among the steps. For example, if you want to train the knowledge graph embedding models, you need to build a knowledge graph or download the pre-built knowledge graph first. If you want to analyze the knowledge graph embedding models, you need to train the knowledge graph embedding models first.","title":"Key Steps in the Project"},{"location":"#step-1-install-dependencies","text":"More details can be found in the Install Dependencies file.","title":"Step 1: Install dependencies"},{"location":"#step-2-build-analyze-a-knowledge-graph","text":"This repository contains the codes to build a knowledge graph for BioMedGPS project. Which depends on the ontology-matcher package and graph-builder package. If you want to run the following codes to build a knowledge graph for BioMedGPS project, you need to install all dependencies first. Please see the Install Dependencies file. After that, you can run the following codes to build a knowledge graph for BioMedGPS project. NOTE: Be sure to activate the python environment you created and located in the root directory of this repository when running the following codes. # Remove the following directories for a clean build rm -rf ./graph_data/extracted_entities ./graph_data/formatted_entities ./graph_data/formatted_relations python run_markdown.py ./graph_data/KG_README.md --run-all # The run_markdown.py is a script to run the codes in a markdown file. # It will extract the code blocks from the markdown file and run them one by one. # If you want to run a specific code block, you can use the following command. # If you see 'Cannot identify the language' message, this means that the code block is not necessary to run. python run_markdown.py ./graph_data/KG_README.md If you want to build a knowledge graph for BioMedGPS project step by step by yourself, you can follow the instructions in the KG_README.md file. How to analyze the knowledge graph? More details can be found in the graph_analysis directory in this repository or see the related documentation graph_analysis/README.md .","title":"Step 2: Build &amp; Analyze a knowledge graph"},{"location":"#deprecated-step-3-train-evaluate-analyze-benchmark-kge-models","text":"NOTE: We're building a new repo for training, evaluating, analyzing and benchmarking knowledge graph embedding / GNN models. Please follow the biomedgps-model repository for the latest updates. Train & evaluate knowledge graph embedding models If you want to train the knowledge graph embedding models by yourself, you can see the training_kge directory in this repository or see the related documentation training_kge/README.md . Benchmark knowledge graph embedding models If you want to benchmark the knowledge graph embedding models, you can see the benchmarks directory in this repository or see the related documentation benchmarks/README.md . Analyze the knowledge graph embedding models If you want to analyze the knowledge graph embedding models, you can see the embedding directory in this repository or see the related documentation embedding/README.md .","title":"[Deprecated] Step 3: Train, Evaluate, Analyze &amp; Benchmark KGE models"},{"location":"#deprecated-step-4-link-prediction","text":"NOTE: We're building a new repo for link prediction. Please follow the biomedgps-model repository for the latest updates. If you want to predict the relations between entities, you can see the prediction directory in this repository or see the related documentation prediction/README.md .","title":"[Deprecated] Step 4: Link Prediction"},{"location":"#deprecated-step-5-explain-the-prediction-results","text":"NOTE: We're building a new repo for explaining the prediction results. Please follow the biomedgps-model repository for the latest updates. More details can be found in the biomedgps-explainer repository.","title":"[Deprecated] Step 5: Explain the prediction results"},{"location":"embeddings_index/","text":"Introduction \u00b6 You may want to use initial embeddings from a pretrained large language model to improve the performance of the model. The idea is that the pretrained large language model has learned the semantic information of the entities and relations from a large corpus. It knows the semantic information of the entities and relations. And in a knowledge graph, many entities might not have enough connections with other entities, we may get a not good embedding for these entities. If we use the initial embeddings instead of the random embeddings, we may make the model learn the semantic information of these entities and relations more easily. In current version, we support two large language models: BioBert and RoBERTa. Actually, We use the transformers library to generate the embeddings. So if you want to use other large language model which is supported by the transformers library, we also support it. You can download the model's weights and refer to it by its path or just use the model name directly. *-400: reduced from 1024-dim embeddings which was generated by the RoBERTa model *-768: generated by the Biobert model *-1024: generated by the RoBERTa model LLM Models \u00b6 BioBert (biobert-base-cased-v1.1) \u00b6 Embeddings from BioBert model have 768 dimensions. You don't need to download the model, just use the model name directly. Because the model is supported by transformers library directly. More details on this model, please visit BioBert or BioBert repository. RoBERTa (RoBERTa-large-PM-M3-Voc) \u00b6 Embeddings from RoBERTa model have 1024 dimensions. You can download the model from here . More details on this model, please visit BioLM repository. Generate Embeddings for Entities \u00b6 mkdir -p embedding/<MODEL_NAME> # If your expected model does not support transformers library directly, you can download the model's weights and refer to it by its path. # We assume you have downloaded entities.tsv file or generated it by following the instructions in the README.md file in the root directory. python embedding/scripts/gen_embeddings.py entities -e graph_data/entities.tsv -m <MODEL_NAME or MODEL_PATH> -o embedding/<MODEL_NAME>/entities_embeddings.tsv # Example: generate embeddings for entities using BioBert model mkdir -p embedding/biobert-base-cased-v1.1 python embedding/scripts/gen_embeddings.py entities -e graph_data/entities.tsv -m dmis-lab/biobert-base-cased-v1.1 -o embedding/biobert-base-cased-v1.1/entities_embeddings.tsv # Example: generate embeddings for entities using RoBERTa model mkdir -p embedding/RoBERTa-large-PM-M3-Voc wget https://dl.fbaipublicfiles.com/biolm/RoBERTa-large-PM-M3-Voc-hf.tar.gz tar -xvf RoBERTa-large-PM-M3-Voc-hf.tar.gz -C embedding/RoBERTa-large-PM-M3-Voc python embedding/scripts/gen_embeddings.py entities -e graph_data/entities.tsv -m ./RoBERTa-large-PM-M3-Voc -o embedding/RoBERTa-large-PM-M3-Voc/entities_embeddings.tsv Generate Embeddings for All Relation Types \u00b6 Descriptions for each relation type \u00b6 More details on relation types, please visit relation_types.tsv file. Generate Embeddings \u00b6 mkdir -p embedding/<MODEL_NAME> python embedding/scripts/gen_embeddings.py relation-types -r embedding/relation_types.tsv -m <MODEL_NAME or MODEL_PATH> -o embedding/<MODEL_NAME>/realtion_types_embeddings.tsv # Example: generate embeddings for relation types using BioBert model mkdir -p embedding/biobert-base-cased-v1.1 python embedding/scripts/gen_embeddings.py relation-types -r embedding/relation_types.tsv -m dmis-lab/biobert-base-cased-v1.1 -o embedding/biobert-base-cased-v1.1/realtion_types_embeddings.tsv # Example: generate embeddings for relation types using RoBERTa model mkdir -p embedding/RoBERTa-large-PM-M3-Voc wget https://dl.fbaipublicfiles.com/biolm/RoBERTa-large-PM-M3-Voc-hf.tar.gz tar -xvf RoBERTa-large-PM-M3-Voc-hf.tar.gz -C embedding/RoBERTa-large-PM-M3-Voc python embedding/scripts/gen_embeddings.py relation-types -r embedding/relation_types.tsv -m ./RoBERTa-large-PM-M3-Voc -o embedding/RoBERTa-large-PM-M3-Voc/realtion_types_embeddings.tsv Visualize Embeddings \u00b6 More details on visualization, please visit visualize.ipynb file in each model folder.","title":"Generating Initial Embeddings"},{"location":"embeddings_index/#introduction","text":"You may want to use initial embeddings from a pretrained large language model to improve the performance of the model. The idea is that the pretrained large language model has learned the semantic information of the entities and relations from a large corpus. It knows the semantic information of the entities and relations. And in a knowledge graph, many entities might not have enough connections with other entities, we may get a not good embedding for these entities. If we use the initial embeddings instead of the random embeddings, we may make the model learn the semantic information of these entities and relations more easily. In current version, we support two large language models: BioBert and RoBERTa. Actually, We use the transformers library to generate the embeddings. So if you want to use other large language model which is supported by the transformers library, we also support it. You can download the model's weights and refer to it by its path or just use the model name directly. *-400: reduced from 1024-dim embeddings which was generated by the RoBERTa model *-768: generated by the Biobert model *-1024: generated by the RoBERTa model","title":"Introduction"},{"location":"embeddings_index/#llm-models","text":"","title":"LLM Models"},{"location":"embeddings_index/#biobert-biobert-base-cased-v11","text":"Embeddings from BioBert model have 768 dimensions. You don't need to download the model, just use the model name directly. Because the model is supported by transformers library directly. More details on this model, please visit BioBert or BioBert repository.","title":"BioBert (biobert-base-cased-v1.1)"},{"location":"embeddings_index/#roberta-roberta-large-pm-m3-voc","text":"Embeddings from RoBERTa model have 1024 dimensions. You can download the model from here . More details on this model, please visit BioLM repository.","title":"RoBERTa (RoBERTa-large-PM-M3-Voc)"},{"location":"embeddings_index/#generate-embeddings-for-entities","text":"mkdir -p embedding/<MODEL_NAME> # If your expected model does not support transformers library directly, you can download the model's weights and refer to it by its path. # We assume you have downloaded entities.tsv file or generated it by following the instructions in the README.md file in the root directory. python embedding/scripts/gen_embeddings.py entities -e graph_data/entities.tsv -m <MODEL_NAME or MODEL_PATH> -o embedding/<MODEL_NAME>/entities_embeddings.tsv # Example: generate embeddings for entities using BioBert model mkdir -p embedding/biobert-base-cased-v1.1 python embedding/scripts/gen_embeddings.py entities -e graph_data/entities.tsv -m dmis-lab/biobert-base-cased-v1.1 -o embedding/biobert-base-cased-v1.1/entities_embeddings.tsv # Example: generate embeddings for entities using RoBERTa model mkdir -p embedding/RoBERTa-large-PM-M3-Voc wget https://dl.fbaipublicfiles.com/biolm/RoBERTa-large-PM-M3-Voc-hf.tar.gz tar -xvf RoBERTa-large-PM-M3-Voc-hf.tar.gz -C embedding/RoBERTa-large-PM-M3-Voc python embedding/scripts/gen_embeddings.py entities -e graph_data/entities.tsv -m ./RoBERTa-large-PM-M3-Voc -o embedding/RoBERTa-large-PM-M3-Voc/entities_embeddings.tsv","title":"Generate Embeddings for Entities"},{"location":"embeddings_index/#generate-embeddings-for-all-relation-types","text":"","title":"Generate Embeddings for All Relation Types"},{"location":"embeddings_index/#descriptions-for-each-relation-type","text":"More details on relation types, please visit relation_types.tsv file.","title":"Descriptions for each relation type"},{"location":"embeddings_index/#generate-embeddings","text":"mkdir -p embedding/<MODEL_NAME> python embedding/scripts/gen_embeddings.py relation-types -r embedding/relation_types.tsv -m <MODEL_NAME or MODEL_PATH> -o embedding/<MODEL_NAME>/realtion_types_embeddings.tsv # Example: generate embeddings for relation types using BioBert model mkdir -p embedding/biobert-base-cased-v1.1 python embedding/scripts/gen_embeddings.py relation-types -r embedding/relation_types.tsv -m dmis-lab/biobert-base-cased-v1.1 -o embedding/biobert-base-cased-v1.1/realtion_types_embeddings.tsv # Example: generate embeddings for relation types using RoBERTa model mkdir -p embedding/RoBERTa-large-PM-M3-Voc wget https://dl.fbaipublicfiles.com/biolm/RoBERTa-large-PM-M3-Voc-hf.tar.gz tar -xvf RoBERTa-large-PM-M3-Voc-hf.tar.gz -C embedding/RoBERTa-large-PM-M3-Voc python embedding/scripts/gen_embeddings.py relation-types -r embedding/relation_types.tsv -m ./RoBERTa-large-PM-M3-Voc -o embedding/RoBERTa-large-PM-M3-Voc/realtion_types_embeddings.tsv","title":"Generate Embeddings"},{"location":"embeddings_index/#visualize-embeddings","text":"More details on visualization, please visit visualize.ipynb file in each model folder.","title":"Visualize Embeddings"},{"location":"faqs/","text":"FAQs and Steps \u00b6 How to prepare a full list of all ontology terms used in the real world? [Complete Ontology List] More details on https://github.com/yjcyxky/biomedical-knowledgebases. Keep curating ontology terms from different databases / papers and integrate them into a full list. If we found a new database which contains new terms, we can download the database and format it into our standard format (more details on graph_data/databases directory), then we can use the ontology-matcher to match the new terms with the full list. If some of the items are not in the full list, they will be added into the full list. How to deal with the terms from different databases but with the same meaning? [ID Integration / Reconciliation] Dealing with the same meaning terms from different databases is a very important issue in the knowledge integration. More details on https://github.com/yjcyxky/ontology-matcher How to integrate knowledges from different databases? [ID Mapping] More details on https://github.com/yjcyxky/biomedical-knowledgebases. Keep curating knowledges from different databases / papers and integrate them into our knowledge graph. If we found a new database which contains new knowledges, we can download the database and format it into our standard format (more details on graph_data/databases directory), then we can use the ontology-matcher to convert the start id and end id into our standard id. If these ids can be converted, we can add the new knowledges into our knowledge graph. How to merge relationship types from different databases? [Relationship Mapping] Define a set of relationship types and map the relationship types from different databases to the set. How to extract the knowledge from the literature? [Text Mining] More details on https://github.com/yjcyxky/text2knowledge","title":"FAQs"},{"location":"faqs/#faqs-and-steps","text":"How to prepare a full list of all ontology terms used in the real world? [Complete Ontology List] More details on https://github.com/yjcyxky/biomedical-knowledgebases. Keep curating ontology terms from different databases / papers and integrate them into a full list. If we found a new database which contains new terms, we can download the database and format it into our standard format (more details on graph_data/databases directory), then we can use the ontology-matcher to match the new terms with the full list. If some of the items are not in the full list, they will be added into the full list. How to deal with the terms from different databases but with the same meaning? [ID Integration / Reconciliation] Dealing with the same meaning terms from different databases is a very important issue in the knowledge integration. More details on https://github.com/yjcyxky/ontology-matcher How to integrate knowledges from different databases? [ID Mapping] More details on https://github.com/yjcyxky/biomedical-knowledgebases. Keep curating knowledges from different databases / papers and integrate them into our knowledge graph. If we found a new database which contains new knowledges, we can download the database and format it into our standard format (more details on graph_data/databases directory), then we can use the ontology-matcher to convert the start id and end id into our standard id. If these ids can be converted, we can add the new knowledges into our knowledge graph. How to merge relationship types from different databases? [Relationship Mapping] Define a set of relationship types and map the relationship types from different databases to the set. How to extract the knowledge from the literature? [Text Mining] More details on https://github.com/yjcyxky/text2knowledge","title":"FAQs and Steps"},{"location":"graph_analysis_index/","text":"Analyze your relation file \u00b6 In this jupyter notebook, we will build a graph based on your relation file and do some analysis on it. Such as the number of nodes, the number of edges, the number of subgraphs, and so on. Based on the metrics, you can know whether your relation file is valid for training or not. If your relation file have too many subgraphs and no any subgraph is large enough (e.g. the percent of the number of nodes and edges in a subgraph is no more than 90% of the total number of nodes and edges in the graph.), you may need to consider to add more relations to your relation file. In our opinion, the number of subgraphs should be as small as possible, and the number of nodes and edges in a subgraph should be as large as possible. In this way, the model can learn more information from the graph. Please refer to the graph_analysis.ipynb for more details. Stat & Visualize your relation file \u00b6 In this jupyter notebook, we will process and visualize statistical data related to entities and relations. Utilizing the Plotly library, it creates charts & tables that showcase statistical information about various types of entities and relationships. These charts & tables are intended to provide users with a quick and comprehensive understanding of the data. Node Count: Displays the count of different types of entities. Each row represents a type of entity, with columns including the resources associated with the entity and their respective counts. Relationship Count: Shows the count of different types of relationships. Each row represents a type of relationship, with columns detailing the resources associated with the relationship and their counts. Please refer to the visualize.ipynb for more details.","title":"Analyzing Knowledge Graph"},{"location":"graph_analysis_index/#analyze-your-relation-file","text":"In this jupyter notebook, we will build a graph based on your relation file and do some analysis on it. Such as the number of nodes, the number of edges, the number of subgraphs, and so on. Based on the metrics, you can know whether your relation file is valid for training or not. If your relation file have too many subgraphs and no any subgraph is large enough (e.g. the percent of the number of nodes and edges in a subgraph is no more than 90% of the total number of nodes and edges in the graph.), you may need to consider to add more relations to your relation file. In our opinion, the number of subgraphs should be as small as possible, and the number of nodes and edges in a subgraph should be as large as possible. In this way, the model can learn more information from the graph. Please refer to the graph_analysis.ipynb for more details.","title":"Analyze your relation file"},{"location":"graph_analysis_index/#stat-visualize-your-relation-file","text":"In this jupyter notebook, we will process and visualize statistical data related to entities and relations. Utilizing the Plotly library, it creates charts & tables that showcase statistical information about various types of entities and relationships. These charts & tables are intended to provide users with a quick and comprehensive understanding of the data. Node Count: Displays the count of different types of entities. Each row represents a type of entity, with columns including the resources associated with the entity and their respective counts. Relationship Count: Shows the count of different types of relationships. Each row represents a type of relationship, with columns detailing the resources associated with the relationship and their counts. Please refer to the visualize.ipynb for more details.","title":"Stat &amp; Visualize your relation file"},{"location":"graph_data_index/","text":"Download the Formatted Data \u00b6 The formatted data is available at Google Drive Now you can download the formatted data and put all data you expected in the graph_data folder. Other codes will read the data from this folder. ID Format \u00b6 Entity ID \u00b6 We use entity_id to represent an entity. It is a string of the following format: <entity_type>:<database_id> such as Gene::ENTREZ:1234 , Disease::OMIM:1234 , Drug::DRUGBANK:DB1234 . Relation ID \u00b6 We use relation_id to represent a relation. It is a string of the following format: <resource>::<relation_type>::<source_entity_type>:<target_entity_type> such as STRING::INTERACTS_WITH::Gene:Gene , STRING::INTERACTS_WITH::Gene:Disease . Data Format \u00b6 Entity File \u00b6 The entity file is a tab-separated file with the following format: id name label resource description synonyms pmids taxid xrefs SYMP:0000149 obsolete sudden onset of severe chills Symptom SymptomOntology SYMP:0000259 dry hacking cough Symptom SymptomOntology A dry cough that is characterized by a rough and loud sound. Relation File \u00b6 The relation file is a tab-separated file with the following format: raw_source_id raw_target_id raw_source_type raw_target_type relation_type resource pmids key_sentence source_id source_type target_id target_type ENTREZ:2261 CHEMBL:CHEMBL100473 Gene Compound DGIDB::OTHER::Gene:Compound DGIDB ENTREZ:2261 Gene MESH:C113580Compound ENTREZ:2776 CHEMBL:CHEMBL100473 Gene Compound DGIDB::OTHER::Gene:Compound DGIDB ENTREZ:2776 Gene MESH:C113580Compound Raw User Defined Knowledge Graph File \u00b6 Raw user defined knowledge graph dataset uses the Raw IDs. The knowledge graph can be stored in a single file (only providing the trainset) or in three files (trainset, validset and testset). Each file stores the triplets of the knowledge graph. The order of head, relation and tail can be arbitry, e.g. [h, r, t]. A delimiter should be used to seperate them. The recommended delimiter is \\t. Gene::ENTREZ:8350 Hetionet::GiG::Gene:Gene Gene::ENTREZ:54583 Gene::ENTREZ:1022 bioarx::HumGenHumGen::Gene:Gene Gene::ENTREZ:890 Gene::ENTREZ:2534 STRING::REACTION::Gene:Gene Gene::ENTREZ:5063 Knowledge Graph File \u00b6 The knowledge graph file is a tab-separated file with the following format: relation_type resource pmids key_sentence source_id source_type target_id target_type source_name target_name DGIDB::INHIBITOR::Gene:Compound DGIDB ENTREZ:4311 Gene MESH:D015244 Compound membrane metalloendopeptidase Thiorphan DGIDB::INHIBITOR::Gene:Compound DGIDB ENTREZ:4311 Gene MESH:C097292 Compound membrane metalloendopeptidase aladotrilat Annotated Knowledge Graph File \u00b6 The annotated knowledge graph file is a tab-separated file with the following format: relation_type resource pmids key_sentence source_id source_type target_id target_type source_name source_description target_name target_description DGIDB::INHIBITOR::Gene:Compound DGIDB ENTREZ:4311 Gene MESH:D015244 Compound membrane metalloendopeptidase The protein encoded by this gene is a type II transmembrane glycoprotein and a common acute lymphocytic leukemia antigen that is an important cell surface marker in the diagnosis of human acute lymphocytic leukemia (ALL). The encoded protein is present on leukemic cells of pre-B phenotype, which represent 85% of cases of ALL. This protein is not restricted to leukemic cells, however, and is found on a variety of normal tissues. The protein is a neutral endopeptidase that cleaves peptides at the amino side of hydrophobic residues and inactivates several peptide hormones including glucagon, enkephalins, substance P, neurotensin, oxytocin, and bradykinin. [provided by RefSeq, Aug 2017]. Thiorphan A potent inhibitor of membrane metalloendopeptidase (ENKEPHALINASE). Thiorphan potentiates morphine-induced ANALGESIA and attenuates naloxone-precipitated withdrawal symptoms. DGIDB::INHIBITOR::Gene:Compound DGIDB ENTREZ:4311 Gene MESH:C097292 Compound membrane metalloendopeptidase The protein encoded by this gene is a type II transmembrane glycoprotein and a common acute lymphocytic leukemia antigen that is an important cell surface marker in the diagnosis of human acute lymphocytic leukemia (ALL). The encoded protein is present on leukemic cells of pre-B phenotype, which represent 85% of cases of ALL. This protein is not restricted to leukemic cells, however, and is found on a variety of normal tissues. The protein is a neutral endopeptidase that cleaves peptides at the amino side of hydrophobic residues and inactivates several peptide hormones including glucagon, enkephalins, substance P, neurotensin, oxytocin, and bradykinin. [provided by RefSeq, Aug 2017]. aladotrilat","title":"Using Knowledge Graph"},{"location":"graph_data_index/#download-the-formatted-data","text":"The formatted data is available at Google Drive Now you can download the formatted data and put all data you expected in the graph_data folder. Other codes will read the data from this folder.","title":"Download the Formatted Data"},{"location":"graph_data_index/#id-format","text":"","title":"ID Format"},{"location":"graph_data_index/#entity-id","text":"We use entity_id to represent an entity. It is a string of the following format: <entity_type>:<database_id> such as Gene::ENTREZ:1234 , Disease::OMIM:1234 , Drug::DRUGBANK:DB1234 .","title":"Entity ID"},{"location":"graph_data_index/#relation-id","text":"We use relation_id to represent a relation. It is a string of the following format: <resource>::<relation_type>::<source_entity_type>:<target_entity_type> such as STRING::INTERACTS_WITH::Gene:Gene , STRING::INTERACTS_WITH::Gene:Disease .","title":"Relation ID"},{"location":"graph_data_index/#data-format","text":"","title":"Data Format"},{"location":"graph_data_index/#entity-file","text":"The entity file is a tab-separated file with the following format: id name label resource description synonyms pmids taxid xrefs SYMP:0000149 obsolete sudden onset of severe chills Symptom SymptomOntology SYMP:0000259 dry hacking cough Symptom SymptomOntology A dry cough that is characterized by a rough and loud sound.","title":"Entity File"},{"location":"graph_data_index/#relation-file","text":"The relation file is a tab-separated file with the following format: raw_source_id raw_target_id raw_source_type raw_target_type relation_type resource pmids key_sentence source_id source_type target_id target_type ENTREZ:2261 CHEMBL:CHEMBL100473 Gene Compound DGIDB::OTHER::Gene:Compound DGIDB ENTREZ:2261 Gene MESH:C113580Compound ENTREZ:2776 CHEMBL:CHEMBL100473 Gene Compound DGIDB::OTHER::Gene:Compound DGIDB ENTREZ:2776 Gene MESH:C113580Compound","title":"Relation File"},{"location":"graph_data_index/#raw-user-defined-knowledge-graph-file","text":"Raw user defined knowledge graph dataset uses the Raw IDs. The knowledge graph can be stored in a single file (only providing the trainset) or in three files (trainset, validset and testset). Each file stores the triplets of the knowledge graph. The order of head, relation and tail can be arbitry, e.g. [h, r, t]. A delimiter should be used to seperate them. The recommended delimiter is \\t. Gene::ENTREZ:8350 Hetionet::GiG::Gene:Gene Gene::ENTREZ:54583 Gene::ENTREZ:1022 bioarx::HumGenHumGen::Gene:Gene Gene::ENTREZ:890 Gene::ENTREZ:2534 STRING::REACTION::Gene:Gene Gene::ENTREZ:5063","title":"Raw User Defined Knowledge Graph File"},{"location":"graph_data_index/#knowledge-graph-file","text":"The knowledge graph file is a tab-separated file with the following format: relation_type resource pmids key_sentence source_id source_type target_id target_type source_name target_name DGIDB::INHIBITOR::Gene:Compound DGIDB ENTREZ:4311 Gene MESH:D015244 Compound membrane metalloendopeptidase Thiorphan DGIDB::INHIBITOR::Gene:Compound DGIDB ENTREZ:4311 Gene MESH:C097292 Compound membrane metalloendopeptidase aladotrilat","title":"Knowledge Graph File"},{"location":"graph_data_index/#annotated-knowledge-graph-file","text":"The annotated knowledge graph file is a tab-separated file with the following format: relation_type resource pmids key_sentence source_id source_type target_id target_type source_name source_description target_name target_description DGIDB::INHIBITOR::Gene:Compound DGIDB ENTREZ:4311 Gene MESH:D015244 Compound membrane metalloendopeptidase The protein encoded by this gene is a type II transmembrane glycoprotein and a common acute lymphocytic leukemia antigen that is an important cell surface marker in the diagnosis of human acute lymphocytic leukemia (ALL). The encoded protein is present on leukemic cells of pre-B phenotype, which represent 85% of cases of ALL. This protein is not restricted to leukemic cells, however, and is found on a variety of normal tissues. The protein is a neutral endopeptidase that cleaves peptides at the amino side of hydrophobic residues and inactivates several peptide hormones including glucagon, enkephalins, substance P, neurotensin, oxytocin, and bradykinin. [provided by RefSeq, Aug 2017]. Thiorphan A potent inhibitor of membrane metalloendopeptidase (ENKEPHALINASE). Thiorphan potentiates morphine-induced ANALGESIA and attenuates naloxone-precipitated withdrawal symptoms. DGIDB::INHIBITOR::Gene:Compound DGIDB ENTREZ:4311 Gene MESH:C097292 Compound membrane metalloendopeptidase The protein encoded by this gene is a type II transmembrane glycoprotein and a common acute lymphocytic leukemia antigen that is an important cell surface marker in the diagnosis of human acute lymphocytic leukemia (ALL). The encoded protein is present on leukemic cells of pre-B phenotype, which represent 85% of cases of ALL. This protein is not restricted to leukemic cells, however, and is found on a variety of normal tissues. The protein is a neutral endopeptidase that cleaves peptides at the amino side of hydrophobic residues and inactivates several peptide hormones including glucagon, enkephalins, substance P, neurotensin, oxytocin, and bradykinin. [provided by RefSeq, Aug 2017]. aladotrilat","title":"Annotated Knowledge Graph File"},{"location":"graph_data_kg/","text":"Entities \u00b6 Extract entities from a set of databases For more convience, we also include all entity items from knowledgebases. First, we extract the id, name, description, label, resource fields from the knowledgebases, and then we map the id to MONDO terms by using the ontology-matcher tool. There will be more easier to integrate these knowledgebases into our knowledge graph, if we can identify the unmapped terms first. If you want to add a new database, please add a new folder in the data directory and add a README.md file to describe how to download the database, write a python script to extract entities from the database. After that, please modify the extract_entities.sh file to run the python script. You can follow the existing scripts as an example. Step1: Extract entities \u00b6 This step will extract entities from a set of databases. The following script will run all the scripts in each folder in the data directory and extract entities. All the extracted entities will be saved in the graph_data/extracted_entities/raw_entities folder. Each database will have a folder in the graph_data/extracted_entities/raw_entities folder. If not, then it means that the database has not extracted entities. If you don't download the related database, you will get an error. # Extract entities from a set of databases # Clean the extracted entities folder rm -rf graph_data/extracted_entities bash graph_data/scripts/extract_entities.sh -t all Step2: Merge entities \u00b6 This step will merge all the entities with the same type into one file. After merged, all the entities will be saved in the graph_data/extracted_entities/merged_entities folder. All the entities with the same type will be deduplicated by id and merged into one file. Each entity type will have a file in the graph_data/extracted_entities/merged_entities folder. # Merge entity files by entity type mkdir -p graph_data/extracted_entities/merged_entities python graph_data/scripts/merge_entities.py from-databases -i graph_data/extracted_entities/raw_entities -o graph_data/extracted_entities/merged_entities Step3: Format entities \u00b6 This step will map the entities to a default ontology and format the entities with a defined fields and format. If you want to add more fields or change the format of a entity id, you can do it at this step. After formatted, all the entities will be saved in the formatted_entities folder. All the entities with the same type will be mapped to a default ontology which is defined in the onto-match package. Each entity type will have a entity file and a pickle file in the formatted_entities folder. The pickle file is the ontology mapping result. If you want to know why your ids is not mapped successfully, you can check the pickle file. NOTE: The format strategy is defined in the onto-match package. If you want to change the format strategy, you can modify the onto-match package. The basic rules are as follows: We select a default ontology for each entity type. For example, we select the MONDO ID for disease, the ENTREZ ID for gene, the DrugBank ID for compound, the HMDB ID for metabolite, the KEGG ID for pathway, the UMLS ID for side-effect, the SYMP ID for symptom, the UMLS ID for anatomy, the GO ID for cellular_component, the GO ID for biological_process, the GO ID for molecular_function, the UMLS ID for pharmacologic_class. We will use the onto-match package to map the ids to the default ontology. NOTE: If the mapping result is not successful, we will use the original id as the ontology id. # Format and filter entities by online ontology service # Clean the formatted entities folder rm -rf graph_data/formatted_entities mkdir graph_data/formatted_entities # More details about the format strategy, please refer to format_entities.sh (We used the onto-match package to map the ids to the default ontology in the format_entities.sh script) bash graph_data/scripts/format_entities.sh # [Optional] Merge several entity types into one new entity type, such as you may want to merge the `Symptom` and `Phenotype` into `Phenotype` sed -i 's/\\tSymptom\\t/\\tPhenotype\\t/g' graph_data/formatted_entities/symptom.tsv Step4: Merge entity files into one file \u00b6 This step will merge all the entity files into one file. If we can find a filtered.tsv file in the formatted_entities folder, we will use the filtered.tsv file to merge entities. Otherwise, we will use the tsv file to merge entities. If you want to keep a subset of entities (such as deduplicating rows with some conditions and filtering rows with specified species etc.), this is a good opportunity to do it. You can do this at the Step3 and save a *.filtered.tsv file in the formatted_entities folder. Finally, the merged file will be saved in the graph_data folder. This file can be the reference file for formatting relations. You will get three files: entities.tsv [after deduplication], entities_full.tsv [before deduplication], entities.log [the log file for deduplication]. NOTE: If you add a new entity type, you should change the merge_entities.py file to add the new entity type. # Merge formatted entity files into one file, we will get three files: entities.tsv [after deduplication], entities_full.tsv [before deduplication], entities.log [the log file for deduplication] python graph_data/scripts/merge_entities.py to-single-file -i graph_data/formatted_entities -o graph_data/entities.tsv --deep-deduplication --remove-obsolete Relations \u00b6 Extract relations from a set of databases \u00b6 # Extract relations from a set of databases ## Clean the formatted relations folder rm -rf graph_data/formatted_relations ## STEP1: The graph-builder tool only supports the following databases CTD, DRKG, PrimeKG, HSDN automatically. Other databases are included in the relations folder. You may need to format them manually by running the main.ipynb files in each subfolder. Like `biosnap`, `cbcg`, `dgidb`, `ttd`. ## STEP2: Run the graph-builder tool to format the preset databases. You might need to prepare a relation_types.tsv file from the relation_types.xlsx file. If you don't want to format the relation types at this step, please don't provide the --relation-type-dict-fpath option. # graph-builder --database ctd --database drkg --database primekg --database hsdn -d ./graph_data/relations -o ./graph_data/formatted_relations -f ./graph_data/entities.tsv -n 20 --download --skip -l ./graph_data/log.txt --debug --relation-type-dict-fpath ./graph_data/relation_types.tsv graph-builder --database ctd --database drkg --database primekg --database hsdn -d ./graph_data/relations -o ./graph_data/formatted_relations -f ./graph_data/entities.tsv -n 20 --download --skip -l ./graph_data/log.txt --debug # graph-builder --database customdb -d ./datasets/biomedgps-v2/knowledge_graph_filtered_corrected_sideeffect.tsv -o ~/Downloads/Test/ -f ./graph_data/entities.tsv -n 20 --download --skip -l ~/Downloads/Test/log.txt --debug --relation-type-dict-fpath ./graph_data/relation_types.tsv Output all of relation types listed in the relation files \u00b6 # Merge formatted relation files into one file python graph_data/scripts/merge_relations.py -i graph_data/formatted_relations -o /tmp/relations.tsv # Output all of relation types listed in the relation files. The relation types are listed in the `relation_type` column. python graph_data/scripts/extract_relation_types.py -i /tmp/relations.tsv -o /tmp/relation_types.tsv Prepare a dataset for training \u00b6 Please refer to the prepare_dataset.ipynb file in the graph_data folder.","title":"Building Knowledge Graph"},{"location":"graph_data_kg/#entities","text":"Extract entities from a set of databases For more convience, we also include all entity items from knowledgebases. First, we extract the id, name, description, label, resource fields from the knowledgebases, and then we map the id to MONDO terms by using the ontology-matcher tool. There will be more easier to integrate these knowledgebases into our knowledge graph, if we can identify the unmapped terms first. If you want to add a new database, please add a new folder in the data directory and add a README.md file to describe how to download the database, write a python script to extract entities from the database. After that, please modify the extract_entities.sh file to run the python script. You can follow the existing scripts as an example.","title":"Entities"},{"location":"graph_data_kg/#step1-extract-entities","text":"This step will extract entities from a set of databases. The following script will run all the scripts in each folder in the data directory and extract entities. All the extracted entities will be saved in the graph_data/extracted_entities/raw_entities folder. Each database will have a folder in the graph_data/extracted_entities/raw_entities folder. If not, then it means that the database has not extracted entities. If you don't download the related database, you will get an error. # Extract entities from a set of databases # Clean the extracted entities folder rm -rf graph_data/extracted_entities bash graph_data/scripts/extract_entities.sh -t all","title":"Step1: Extract entities"},{"location":"graph_data_kg/#step2-merge-entities","text":"This step will merge all the entities with the same type into one file. After merged, all the entities will be saved in the graph_data/extracted_entities/merged_entities folder. All the entities with the same type will be deduplicated by id and merged into one file. Each entity type will have a file in the graph_data/extracted_entities/merged_entities folder. # Merge entity files by entity type mkdir -p graph_data/extracted_entities/merged_entities python graph_data/scripts/merge_entities.py from-databases -i graph_data/extracted_entities/raw_entities -o graph_data/extracted_entities/merged_entities","title":"Step2: Merge entities"},{"location":"graph_data_kg/#step3-format-entities","text":"This step will map the entities to a default ontology and format the entities with a defined fields and format. If you want to add more fields or change the format of a entity id, you can do it at this step. After formatted, all the entities will be saved in the formatted_entities folder. All the entities with the same type will be mapped to a default ontology which is defined in the onto-match package. Each entity type will have a entity file and a pickle file in the formatted_entities folder. The pickle file is the ontology mapping result. If you want to know why your ids is not mapped successfully, you can check the pickle file. NOTE: The format strategy is defined in the onto-match package. If you want to change the format strategy, you can modify the onto-match package. The basic rules are as follows: We select a default ontology for each entity type. For example, we select the MONDO ID for disease, the ENTREZ ID for gene, the DrugBank ID for compound, the HMDB ID for metabolite, the KEGG ID for pathway, the UMLS ID for side-effect, the SYMP ID for symptom, the UMLS ID for anatomy, the GO ID for cellular_component, the GO ID for biological_process, the GO ID for molecular_function, the UMLS ID for pharmacologic_class. We will use the onto-match package to map the ids to the default ontology. NOTE: If the mapping result is not successful, we will use the original id as the ontology id. # Format and filter entities by online ontology service # Clean the formatted entities folder rm -rf graph_data/formatted_entities mkdir graph_data/formatted_entities # More details about the format strategy, please refer to format_entities.sh (We used the onto-match package to map the ids to the default ontology in the format_entities.sh script) bash graph_data/scripts/format_entities.sh # [Optional] Merge several entity types into one new entity type, such as you may want to merge the `Symptom` and `Phenotype` into `Phenotype` sed -i 's/\\tSymptom\\t/\\tPhenotype\\t/g' graph_data/formatted_entities/symptom.tsv","title":"Step3: Format entities"},{"location":"graph_data_kg/#step4-merge-entity-files-into-one-file","text":"This step will merge all the entity files into one file. If we can find a filtered.tsv file in the formatted_entities folder, we will use the filtered.tsv file to merge entities. Otherwise, we will use the tsv file to merge entities. If you want to keep a subset of entities (such as deduplicating rows with some conditions and filtering rows with specified species etc.), this is a good opportunity to do it. You can do this at the Step3 and save a *.filtered.tsv file in the formatted_entities folder. Finally, the merged file will be saved in the graph_data folder. This file can be the reference file for formatting relations. You will get three files: entities.tsv [after deduplication], entities_full.tsv [before deduplication], entities.log [the log file for deduplication]. NOTE: If you add a new entity type, you should change the merge_entities.py file to add the new entity type. # Merge formatted entity files into one file, we will get three files: entities.tsv [after deduplication], entities_full.tsv [before deduplication], entities.log [the log file for deduplication] python graph_data/scripts/merge_entities.py to-single-file -i graph_data/formatted_entities -o graph_data/entities.tsv --deep-deduplication --remove-obsolete","title":"Step4: Merge entity files into one file"},{"location":"graph_data_kg/#relations","text":"","title":"Relations"},{"location":"graph_data_kg/#extract-relations-from-a-set-of-databases","text":"# Extract relations from a set of databases ## Clean the formatted relations folder rm -rf graph_data/formatted_relations ## STEP1: The graph-builder tool only supports the following databases CTD, DRKG, PrimeKG, HSDN automatically. Other databases are included in the relations folder. You may need to format them manually by running the main.ipynb files in each subfolder. Like `biosnap`, `cbcg`, `dgidb`, `ttd`. ## STEP2: Run the graph-builder tool to format the preset databases. You might need to prepare a relation_types.tsv file from the relation_types.xlsx file. If you don't want to format the relation types at this step, please don't provide the --relation-type-dict-fpath option. # graph-builder --database ctd --database drkg --database primekg --database hsdn -d ./graph_data/relations -o ./graph_data/formatted_relations -f ./graph_data/entities.tsv -n 20 --download --skip -l ./graph_data/log.txt --debug --relation-type-dict-fpath ./graph_data/relation_types.tsv graph-builder --database ctd --database drkg --database primekg --database hsdn -d ./graph_data/relations -o ./graph_data/formatted_relations -f ./graph_data/entities.tsv -n 20 --download --skip -l ./graph_data/log.txt --debug # graph-builder --database customdb -d ./datasets/biomedgps-v2/knowledge_graph_filtered_corrected_sideeffect.tsv -o ~/Downloads/Test/ -f ./graph_data/entities.tsv -n 20 --download --skip -l ~/Downloads/Test/log.txt --debug --relation-type-dict-fpath ./graph_data/relation_types.tsv","title":"Extract relations from a set of databases"},{"location":"graph_data_kg/#output-all-of-relation-types-listed-in-the-relation-files","text":"# Merge formatted relation files into one file python graph_data/scripts/merge_relations.py -i graph_data/formatted_relations -o /tmp/relations.tsv # Output all of relation types listed in the relation files. The relation types are listed in the `relation_type` column. python graph_data/scripts/extract_relation_types.py -i /tmp/relations.tsv -o /tmp/relation_types.tsv","title":"Output all of relation types listed in the relation files"},{"location":"graph_data_kg/#prepare-a-dataset-for-training","text":"Please refer to the prepare_dataset.ipynb file in the graph_data folder.","title":"Prepare a dataset for training"},{"location":"installation_index/","text":"Install Dependencies \u00b6 NOTE: 1. Python >=3.10 is required. 2. All scripts in the repository are dependent on the following dependencies. If you want to run the scripts/jupyter notebooks in this repository, you need to install all dependencies first. In addition, you need to specify the python kernel in the jupyter notebook to the python environment you created when running a jupyter notebook in this repository. We assume that you have download/clone this repository to your local machine. If not, please download/clone this repository to your local machine first. # Clone the repository git clone https://github.com/open-prophetdb/biomedgps-data cd biomedgps-data We recommend you to use virtualenv or conda to install the dependencies. If you don't have virtualenv or conda installed, you can install them by following the instructions in the official document. # [Option 1] Install the dependencies with virtualenv virtualenv -p python3 .env source .env/bin/activate # [Option 2] Install the dependencies with conda conda create -n biomedgps-data python=3.10 conda activate biomedgps-data # Install the dependencies pip install -r requirements.txt","title":"Installation"},{"location":"installation_index/#install-dependencies","text":"NOTE: 1. Python >=3.10 is required. 2. All scripts in the repository are dependent on the following dependencies. If you want to run the scripts/jupyter notebooks in this repository, you need to install all dependencies first. In addition, you need to specify the python kernel in the jupyter notebook to the python environment you created when running a jupyter notebook in this repository. We assume that you have download/clone this repository to your local machine. If not, please download/clone this repository to your local machine first. # Clone the repository git clone https://github.com/open-prophetdb/biomedgps-data cd biomedgps-data We recommend you to use virtualenv or conda to install the dependencies. If you don't have virtualenv or conda installed, you can install them by following the instructions in the official document. # [Option 1] Install the dependencies with virtualenv virtualenv -p python3 .env source .env/bin/activate # [Option 2] Install the dependencies with conda conda create -n biomedgps-data python=3.10 conda activate biomedgps-data # Install the dependencies pip install -r requirements.txt","title":"Install Dependencies"}]}