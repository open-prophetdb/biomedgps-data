PATH_TRAINING = data/<dataset_name>/train.txt

PATH_OUTPUT   = rules

SNAPSHOTS_AT = 10,50,100

# If you have 24 virtual cores on your computing device, set WORKER_THREADS = 22 for example.
WORKER_THREADS = 7

# Policy: POLICY = 2 is the default setting. Possible values are 1 (greedy policy) and 2 (weighted policy). Experiments do not show a significant difference even tough the weighted variant is probably more robust in the sense that its less negative affected by the specifics of rather specific datasets.
POLICY = 2

# Reward: REWARD = 5 is the default setting. Possible values are 1 (correct predictions), 3 (correct predictions weighted by confidence with laplace smoothing), 5 (correct predictions weighted by confidence with laplace smoothing divided by (rule length-1)^2).
REWARD = 5

# Epsilon: EPSILON = 0.1 is the default setting, which allocates a core with a probability of 0.1 randomly. You can change this to a value of 0.0 to 1.0 (= random policy).
EPSILON = 0.1

# Thresholds for learning rules can be set like this THRESHOLD_CORRECT_PREDICTIONS = 2 and or THRESHOLD_CONFIDENCE = 0.0001. The values shown here are the default values.
THRESHOLD_CORRECT_PREDICTIONS = 2
THRESHOLD_CONFIDENCE = 0.0001

# called binary rules or cyclic rules
MAX_LENGTH_CYCLIC = 3 

# rules that try to capture very simple relation specific frequencies
ZERO_RULES_ACTIVE = false 

# these rules are called AC1 and AC2 rule or U_c or U_d rules (unary)
MAX_LENGTH_ACYCLIC = 1 
 
# U_d or AC1 rules that are derived from a closed path
MAX_LENGTH_GROUNDED_CYCLIC = 1