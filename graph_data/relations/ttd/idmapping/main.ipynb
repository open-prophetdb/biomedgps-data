{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://db.idrblab.net/ttd/sites/default/files/ttd_database/P1-03-TTD_crossmatching.txt -O P1-03-TTD_crossmatching.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drug ID Mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV file has been saved to ttd_drug_id.tsv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "def parse_section(section):\n",
    "    \"\"\"从给定section中解析TTDDRUID，CHEBI_ID和PUBCHCID\"\"\"\n",
    "    section_data = {}\n",
    "    for line in section.split(\"\\n\"):\n",
    "        parts = line.split(\"\\t\")\n",
    "        # 确保数据行格式正确\n",
    "        if len(parts) == 3:\n",
    "            section_data[parts[1]] = parts[2]\n",
    "\n",
    "    return section_data\n",
    "\n",
    "\n",
    "def construct_drug_id(chebi_id, pubchcid):\n",
    "    \"\"\"构造Drug_id列的值\"\"\"\n",
    "    if pubchcid:\n",
    "        return f\"PUBCHEM:{pubchcid}\"\n",
    "    elif chebi_id:\n",
    "        return chebi_id\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "\n",
    "def process_file(file_path):\n",
    "    # 读取文件并按空行分割sections\n",
    "    with open(file_path, \"r\") as file:\n",
    "        for i in range(0, 28):\n",
    "            next(file)\n",
    "\n",
    "        sections = file.read().strip().split(\"\\n\\n\")\n",
    "\n",
    "    data = []\n",
    "    for section in sections:\n",
    "        section_data = parse_section(section)\n",
    "        drug_id = construct_drug_id(\n",
    "            section_data.get(\"CHEBI_ID\"), section_data.get(\"PUBCHCID\")\n",
    "        )\n",
    "        data.append({\"TTDDRUID\": section_data.get(\"TTDDRUID\", \"\"), \"Drug_id\": drug_id})\n",
    "\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "file_path = \"./P1-03-TTD_crossmatching.txt\"\n",
    "df = process_file(file_path)\n",
    "\n",
    "output_path = \"processed_ttd_drug_id.tsv\"\n",
    "invalid = df[df[\"Drug_id\"].isnull()]\n",
    "df = df[df[\"Drug_id\"].notnull()]\n",
    "\n",
    "invalid.to_csv(\"invalid_ttd_drug_id.tsv\", sep=\"\\t\", index=False)\n",
    "df.to_csv(output_path, index=False, sep=\"\\t\")\n",
    "\n",
    "print(f\"CSV file has been saved to {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Biomarker ID Mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch Biomarker data from TTD\n",
    "import requests, re\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def fetch_biomarker(biomarker_id: str):\n",
    "    url = f\"https://db.idrblab.net/ttd/data/biomarker/details/{biomarker_id}\"\n",
    "\n",
    "    # 使用requests获取页面内容\n",
    "    response = requests.get(url)\n",
    "\n",
    "    # 确保请求成功\n",
    "    if response.status_code == 200:\n",
    "        # 使用BeautifulSoup解析HTML内容\n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "        # 找到表格，假设它是页面中的第一个表格\n",
    "        table = soup.find(\"table\")\n",
    "\n",
    "        # 解析表格，提取标题行和数据行\n",
    "        headers = [header.text for header in table.find_all(\"th\")]\n",
    "        rows = []\n",
    "        for row in table.find_all(\"tr\"):\n",
    "            columns = row.find_all(\"td\")\n",
    "            rows.append([column.text for column in columns])\n",
    "\n",
    "        # 删除空行\n",
    "        rows = [row for row in rows if len(row) > 0]\n",
    "\n",
    "        # 打印表格数据\n",
    "        headers = list(\n",
    "            filter(\n",
    "                lambda x: x not in [\"Biomarker General Infomation\", \"References\"], headers\n",
    "            )\n",
    "        )\n",
    "        rows = [re.sub(r'\\s+', ' ', row[0].replace(\"\\n\", \"\").strip()) for row in rows if len(row) > 0]\n",
    "        biomarker_dict = dict(zip(headers, rows))\n",
    "    else:\n",
    "        print(\"Failed to retrieve the webpage\")\n",
    "        biomarker_dict = {}\n",
    "\n",
    "    return biomarker_dict\n",
    "\n",
    "\n",
    "def save(biomarker_data, output_path):\n",
    "    d = biomarker_data.values()\n",
    "    # print(d)\n",
    "    biomarker_df = pd.DataFrame(list(d))\n",
    "    biomarker_df.to_csv(output_path, index=False, sep=output_path.endswith(\".tsv\") and \"\\t\" or \",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2512/2512 [00:00<00:00, 3367239.26it/s]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import tqdm\n",
    "import os\n",
    "from time import sleep\n",
    "\n",
    "input_path = \"../biomarker-disease/P1-08-Biomarker_disease_extracted.tsv\"\n",
    "output_path = \"./processed_ttd_biomarkers.tsv\"\n",
    "\n",
    "data = pd.read_csv(input_path, sep=\"\\t\")\n",
    "biomarker_ids = data[\"ttd_biomarker_id\"].unique()\n",
    "\n",
    "biomarker_data = (\n",
    "    pd.read_csv(output_path, sep=\"\\t\")\n",
    "    if os.path.exists(output_path)\n",
    "    else pd.DataFrame()\n",
    ")\n",
    "biomarker_data_dict_lst = biomarker_data.to_dict(orient=\"records\")\n",
    "if len(biomarker_data_dict_lst) > 0:\n",
    "    biomarker_data = {biomarker[\"Biomarker ID\"]: biomarker for biomarker in biomarker_data_dict_lst}\n",
    "else:\n",
    "    biomarker_data = {}\n",
    "\n",
    "idx = 1\n",
    "for biomarker_id in tqdm.tqdm(biomarker_ids):\n",
    "    if biomarker_id in biomarker_data:\n",
    "        continue\n",
    "    else:\n",
    "        biomarker_data[biomarker_id] = fetch_biomarker(biomarker_id)\n",
    "        sleep(2)\n",
    "\n",
    "    if idx % 10 == 0 or idx == len(biomarker_ids):\n",
    "        save(biomarker_data, output_path)\n",
    "\n",
    "    idx += 1\n",
    "\n",
    "save(biomarker_data, output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "biomarkers = pd.read_csv(output_path, sep=\"\\t\")\n",
    "biomarkers.head()\n",
    "\n",
    "biomarkers = biomarkers[[\"Biomarker ID\", \"Biomarker Name\", \"UniProt ID\"]]\n",
    "invalid = biomarkers[biomarkers[\"UniProt ID\"].isnull()]\n",
    "biomarkers = biomarkers[biomarkers[\"UniProt ID\"].notnull()]\n",
    "biomarkers[\"UniProt ID\"] = biomarkers[\"UniProt ID\"].apply(lambda x: x.replace(\" \", \"\").split(\";\"))\n",
    "biomarkers = biomarkers.explode(column=\"UniProt ID\")\n",
    "\n",
    "invalid.to_csv(\"invalid_ttd_biomarker_id.tsv\", sep=\"\\t\", index=False)\n",
    "biomarkers.to_csv(\"processed_ttd_biomarker_id.tsv\", sep=\"\\t\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gene ID Mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-04-17 06:01:49--  https://db.idrblab.net/ttd/sites/default/files/ttd_database/P1-01-TTD_target_download.txt\n",
      "Resolving db.idrblab.net (db.idrblab.net)... 47.88.56.212\n",
      "Connecting to db.idrblab.net (db.idrblab.net)|47.88.56.212|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 8672051 (8.3M) [text/plain]\n",
      "Saving to: ‘P1-01-TTD_target_download.txt’\n",
      "\n",
      "P1-01-TTD_target_do 100%[===================>]   8.27M  6.62MB/s    in 1.3s    \n",
      "\n",
      "2024-04-17 06:01:51 (6.62 MB/s) - ‘P1-01-TTD_target_download.txt’ saved [8672051/8672051]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://db.idrblab.net/ttd/sites/default/files/ttd_database/P1-01-TTD_target_download.txt -O P1-01-TTD_target_download.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It's a special case:  ['CAMLG', 'CAML'] ['CAMLG_HUMAN']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "def parse_section(section):\n",
    "    \"\"\"\n",
    "    Parse the given section and return a list of dictionaries containing the Target ID, Target Name, and UniProt ID.\n",
    "    \"\"\"\n",
    "    base_data = {\n",
    "        \"ttd_target_id\": \"\",\n",
    "        \"target_name\": \"\",\n",
    "        \"target_id\": \"\",\n",
    "        \"ttd_uniprot_id\": \"\",\n",
    "        \"target_type\": \"\",\n",
    "        \"synonyms\": \"\",\n",
    "        \"function\": \"\",\n",
    "        \"bio_class\": \"\",\n",
    "        \"ec_number\": \"\",\n",
    "        \"sequence\": \"\",\n",
    "    }\n",
    "\n",
    "    lines = section.split(\"\\n\")\n",
    "    for line in lines:\n",
    "        parts = line.split(\"\\t\")\n",
    "        if parts[1] == \"TARGETID\":\n",
    "            base_data[\"ttd_target_id\"] = parts[2]\n",
    "        elif parts[1] == \"TARGETNAME\" or parts[1] == \"TARGNAME\":\n",
    "            base_data[\"target_name\"] = parts[2]\n",
    "        elif parts[1] == \"GENENAME\":\n",
    "            base_data[\"target_id\"] = parts[2].split(\"; \")\n",
    "        elif parts[1] == \"TARGETTYPE\" or parts[1] == \"TARGTYPE\":\n",
    "            base_data[\"target_type\"] = parts[2]\n",
    "        elif parts[1] == \"SYNONYMS\":\n",
    "            base_data[\"synonyms\"] = parts[2]\n",
    "        elif parts[1] == \"UNIPROID\":\n",
    "            base_data[\"ttd_uniprot_id\"] = parts[2].split(\"; \")\n",
    "        elif parts[1] == \"FUNCTION\":\n",
    "            base_data[\"function\"] = parts[2]\n",
    "        elif parts[1] == \"BIOCLASS\":\n",
    "            base_data[\"bio_class\"] = parts[2]\n",
    "        elif parts[1] == \"ECNUMBER\":\n",
    "            base_data[\"ec_number\"] = parts[2]\n",
    "        elif parts[1] == \"SEQUENCE\":\n",
    "            base_data[\"sequence\"] = parts[2]\n",
    "\n",
    "    return base_data\n",
    "\n",
    "\n",
    "def process_file(filepath):\n",
    "    \"\"\"\n",
    "    Process the given TTD target download file and return a DataFrame containing the extracted data.\n",
    "    \"\"\"\n",
    "    with open(filepath, \"r\") as file:\n",
    "        for i in range(0, 32):\n",
    "            next(file)\n",
    "\n",
    "        content = file.read().strip()\n",
    "        sections = content.split(\"\\n\\n\")\n",
    "\n",
    "    all_data = []\n",
    "    for section in sections:\n",
    "        section_data = parse_section(section)\n",
    "\n",
    "        target_id = section_data.get(\"target_id\", [])\n",
    "        if isinstance(target_id, list):\n",
    "            if len(target_id) > 1:\n",
    "                for idx, id in enumerate(target_id):\n",
    "                    d = section_data.copy()\n",
    "                    try:\n",
    "                        d[\"target_id\"] = \"SYMBOL:\" + id\n",
    "                        d[\"ttd_uniprot_id\"] = d[\"ttd_uniprot_id\"][idx]\n",
    "                    except IndexError:\n",
    "                        print(\n",
    "                            \"It's a special case: \",\n",
    "                            section_data[\"target_id\"],\n",
    "                            section_data[\"ttd_uniprot_id\"],\n",
    "                        )\n",
    "                        d[\"target_id\"] = \"SYMBOL:\" + d[\"target_id\"][0]\n",
    "                        d[\"ttd_uniprot_id\"] = d[\"ttd_uniprot_id\"][0]\n",
    "\n",
    "                    all_data.append(d)\n",
    "            else:\n",
    "                section_data[\"target_id\"] = \"SYMBOL:\" + target_id[0]\n",
    "                section_data[\"ttd_uniprot_id\"] = section_data[\"ttd_uniprot_id\"][0]\n",
    "                all_data.append(section_data)\n",
    "        else:\n",
    "            section_data[\"target_id\"] = \"SYMBOL:\" + target_id\n",
    "            all_data.append(section_data)\n",
    "\n",
    "    df = pd.DataFrame(all_data)\n",
    "\n",
    "    invalid_rows = df[df[\"target_id\"].isna() | df[\"target_id\"].str.contains(\" \")]\n",
    "    invalid_rows.to_csv(\"./invalid_ttd_gene_id.tsv\", index=False, sep=\"\\t\")\n",
    "    df.to_csv(\"./processed_ttd_gene_id.tsv\", index=False, sep=\"\\t\")\n",
    "\n",
    "\n",
    "# 调用process_file函数处理文件\n",
    "process_file(\"P1-01-TTD_target_download.txt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "biomedgps-data",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
